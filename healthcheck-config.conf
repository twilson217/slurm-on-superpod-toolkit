# Slurm Healthcheck Configuration File for BCM-Managed Clusters
# 
# This file allows you to customize the behavior of the healthcheck script.
# Uncomment and modify settings as needed.
#
# NOTE: This script is designed to run on BCM head nodes as root with:
# - Passwordless SSH access to all cluster nodes
# - Shared storage at /cm/shared/ available to all nodes
# - cmsh available at /cm/local/apps/cmd/bin/cmsh

[general]
# Default timeout for command execution (seconds)
command_timeout = 30

# Enable/disable colored output by default
use_colors = true

# Default verbosity level (false = normal, true = verbose)
verbose = false

[bcm]
# BCM-specific settings
# cmsh path (auto-detected if not specified)
cmsh_path = /cm/local/apps/cmd/bin/cmsh

# BCM version (auto-detected from /cm/local/apps/cmd/etc/cmd.conf)
# bcm_version = 10

# Slurm base path (auto-detected based on BCM version)
# BCM 10.x uses: /cm/shared/apps/slurm
# BCM 11.x uses: /cm/local/apps/slurm
# slurm_base_path = /cm/shared/apps/slurm

[checks]
# Enable or disable specific check categories
# Set to false to skip a category of checks

enable_service_checks = true
enable_node_checks = true
enable_accounting_checks = true
enable_job_submission_tests = true
enable_pyxis_checks = true
enable_log_analysis = true
enable_munge_checks = true

[node_checks]
# Node health thresholds
# Percentage of nodes that can be down before triggering FAIL vs WARN
warn_threshold_percent = 10
fail_threshold_percent = 25

[log_checks]
# Time window for log analysis (hours)
# Only checks for errors in this recent time period
log_time_window_hours = 8

# Log files to check (on controller/accounting nodes, accessed via SSH)
slurmctld_log = /var/log/slurm/slurmctld.log
slurmdbd_log = /var/log/slurm/slurmdbd.log

# Note: Logs are checked on the controller and accounting nodes
# discovered via cmsh, not on the local head node
# Uses journalctl with --since "8 hours ago" for time-based filtering

[job_tests]
# Enable/disable actual job submission tests
# Set to false to skip potentially disruptive tests
enable_job_submission = true

# Job test timeout (seconds)
job_test_timeout = 120

[pyxis]
# Pyxis plugin search paths (auto-detected based on BCM version)
# BCM 10.x: /cm/shared/apps/slurm/current/lib/slurm/spank_pyxis.so
# BCM 11.x: /cm/local/apps/slurm/current/lib/slurm/spank_pyxis.so
plugin_paths = [
    "/cm/shared/apps/slurm/current/lib/slurm/spank_pyxis.so",
    "/cm/local/apps/slurm/current/lib/slurm/spank_pyxis.so",
    "/usr/lib/slurm/spank_pyxis.so",
    "/usr/lib64/slurm/spank_pyxis.so"
]

# Enroot is available on all nodes via shared storage /cm/shared/

[baseline]
# Default baseline file location
default_baseline_dir = /var/lib/slurm-healthcheck/baselines

# Baseline retention (days)
baseline_retention_days = 90

[accounting]
# Job history lookback period for baseline capture (days)
job_history_days = 30

# Minimum number of accounting records expected
# Set to 0 to disable minimum checks
min_users = 1
min_accounts = 1
min_clusters = 1

# Database backup directory
# Directory where Slurm accounting database backups are stored
# Healthcheck will verify a recent backup exists before upgrades
slurm_db_backup_dir = /tmp/test-backup

# Maximum age for backup to be considered valid (hours)
# Backup older than this will trigger a warning
max_backup_age_hours = 24

[alerts]
# Email notifications (future feature)
# enable_email_alerts = false
# alert_email = admin@example.com
# smtp_server = localhost

[advanced]
# Additional pdsh options for multi-node checks
# pdsh_options = "-w"

# Expected node count (0 = auto-detect)
expected_node_count = 0

# Critical services that must be running
critical_services = ["slurmctld", "slurmdbd", "munge"]

